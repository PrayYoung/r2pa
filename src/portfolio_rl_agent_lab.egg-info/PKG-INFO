Metadata-Version: 2.4
Name: portfolio-rl-agent-lab
Version: 0.1.0
Summary: Add your description here
Requires-Python: <3.13,>=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: gymnasium>=1.2.3
Requires-Dist: matplotlib>=3.10.8
Requires-Dist: numpy>=2.4.1
Requires-Dist: pandas>=3.0.0
Requires-Dist: pyarrow>=23.0.0
Requires-Dist: requests>=2.32.5
Requires-Dist: sentence-transformers>=5.2.2
Requires-Dist: stable-baselines3>=2.7.1
Requires-Dist: tensorboard>=2.20.0
Requires-Dist: torch>=2.10.0
Requires-Dist: tqdm>=4.67.1
Requires-Dist: yfinance>=1.1.0
Dynamic: license-file

# Portfolio-RL-Agent-Lab

A research-oriented sandbox for building and evaluating a portfolio allocation agent trained with reinforcement learning (PPO), with an extensible “Regime Oracle” interface (heuristic / local vLLM) that produces structured regime features used by the RL policy.

## What’s in this repo

- **RL portfolio environment**: daily multi-asset allocation with transaction costs
- **Training**: PPO (Stable-Baselines3)
- **Evaluation**: backtest + benchmarks + diagnostics
- **Regime Oracle (pluggable)**:
  - heuristic oracle (rule-based)
  - local vLLM oracle (OpenAI-compatible endpoint)

## Quickstart (uv)

```bash
uv venv --python 3.12
source .venv/bin/activate
uv sync
```

Build offline regime features (heuristic)
```bash
uv run python -m portfolio_rl_agent_lab.llm.build_regime_features
```

Train
```bash
uv run python -m portfolio_rl_agent_lab.train.train_ppo
```

Evaluate
```bash
uv run python -m portfolio_rl_agent_lab.eval.benchmarks
uv run python -m portfolio_rl_agent_lab.eval.diagnostics
```

## Notes

- Large artifacts are intentionally excluded from git: artifacts/, .venv/.
- The Regime Oracle is designed to be swappable without changing the RL env/policy code.
